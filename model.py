import pandas as pd
import libraries
from libraries import *

class LRmodel: #Our model: Logistic Regression
    @ignore_warnings(category=ConvergenceWarning)
    def __init__(self, random_state = 42):
        self.pr = preprocess() #Our preprocess class
        self.random_state_pos = random_state
        self.data = libraries.data

        X = self.data.iloc[:, 0:self.data.iloc[0].__len__() - 1] #Seperating X
        y = self.data.loc[:, "tag"] #Seperating Y

        self.model = LogisticRegression(random_state=self.random_state_pos) #Model
        #self.model.fit(X, y) #Fitting
        #pd.DataFrame(self.modelWeights).to_csv("modelWeights.csv")
        #self.model.coef_ = pd.read_csv("modelWeights.csv", index_col=0).values
        self.model.coef_ = np.array([[0.0036, -0.0032, 0.0031, -0.0130, 0.0449, -0.0359, -0.2400, -0.4671, 0.1625, 0.0796, 0.2081, 0.3023, 0.2332, 0.2334, 0.6449, 0.9746, 0.2306, 0.2490, 0.5666, -0.0095, -0.0147, -0.0259, 0.0958, -0.0432, -0.2909, -0.5514, 0.1982, 0.0805, 0.1739, 0.2382, 0.2277, 0.2086, 0.5748, 0.8592, 0.1587, 0.2260, 0.5089, -0.0153, -0.0236, 0.1717, -0.0333, -0.3211, -0.6093, 0.2104, 0.0908, 0.1263, 0.1373, 0.1618, 0.1672, 0.4457, 0.6438, 0.0603, 0.1797, 0.3922, 0.0104, 0.2073, 0.0008, -0.3207, -0.6399, 0.1918, 0.1229, 0.0907, 0.0375, 0.1268, 0.1582, 0.3354, 0.4377, 0.0322, 0.1790, 0.3119, 0.0364, -0.0829, -0.4029, -0.7272, 0.0374, 0.0394, -0.0424, -0.1304, -0.0007, 0.0442, 0.1172, 0.1363, -0.0766, 0.0733, 0.1224, -0.0970, -0.2443, -0.3697, -0.0488, -0.0637, 0.0385, 0.0649, -0.1276, -0.0627, 0.1828, 0.3388, -0.2128, -0.0875, 0.0449, -0.0629, -0.0955, -0.1137, 0.0551, -0.0356, -0.0995, -0.1404, -0.0366, -0.0754, -0.0810, -0.1841, -0.0268, -0.0663, -0.0207, -0.2036, 0.1285, 0.0909, -0.0258, -0.1973, 0.0353, -0.0777, -0.1535, -0.2275, 0.0171, -0.0431, -0.2905, 0.1366, 0.1883, 0.1269, -0.2680, 0.0678, -0.0515, -0.1438, -0.2960, 0.0226, -0.0316, -0.0944, 0.0285, 0.0719, -0.0701, -0.0393, 0.2657, 0.4285, -0.1405, -0.0516, 0.0839, -0.0357, -0.0437, -0.1222, -0.1008, -0.2052, -0.1719, -0.1502, -0.0551, -0.1805, -0.0053, 0.0060, -0.0188, -0.2197, -0.3656, -0.0095, 0.0697, -0.0562, 0.0806, 0.0233, -0.1472, -0.3580, 0.0885, 0.1513, 0.0450, -0.0139, 0.3455, 0.5773, -0.0647, -0.0023, 0.1968, 0.0513, 0.1192, -0.0099, 0.0594, -0.1121, 0.0644, 0.3606, 0.3285, 0.1995, 0.6523, 0.4960, 0.4315, 0.0022, 0.2606, 0.0644]
            ,[-0.0885, -0.1081, -0.2497, -0.3104, 0.1245, 0.0544, 0.0455, 0.0835, 0.1988, 0.1063, 0.2066, 0.3178, 0.2036, -0.0187, -0.1711, -0.2335, 0.1183, -0.1499, -0.2960, 0.0131, -0.0362, -0.0224, 0.2439, 0.1357, 0.1155, 0.1459, 0.3290, 0.2181, 0.3052, 0.4056, 0.2911, 0.0294, -0.1745, -0.2649, 0.1270, -0.1595, -0.3509, 0.0499, 0.1267, 0.1689, 0.0430, 0.0195, 0.0463, 0.2100, 0.1333, 0.1969, 0.2712, 0.0857, -0.1305, -0.3816, -0.4705, -0.0846, -0.3489, -0.5396, 0.0785, -0.0411, 0.0332, 0.0214, 0.0501, -0.2678, -0.0331, 0.0421, 0.1079, -0.4222, -0.4755, -0.7792, -0.7721, -0.5242, -0.7503, -0.8645, -0.0560, 0.2167, 0.1842, 0.1937, -0.5068, -0.1196, -0.0224, 0.0391, -0.8006, -0.8021, -0.9127, -0.6883, -0.8165, -1.0656, -0.9111, -0.1364, -0.1763, -0.1657, 0.0485, -0.0357, -0.0043, 0.0189, 0.0178, -0.2489, -0.4081, -0.3078, -0.0361, -0.3294, -0.3673, -0.0474, -0.0452, -0.0818, 0.2152, 0.0892, 0.0472, -0.0120, -0.0019, -0.0686, -0.0018, 0.0078, -0.0734, -0.0641, 0.0009, -0.1367, 0.1426, 0.1416, 0.0503, -0.0479, 0.0571, 0.0200, 0.0658, 0.0069, 0.0041, 0.0225, -0.1339, 0.0763, 0.1172, 0.0602, -0.0372, 0.1018, 0.0872, 0.1243, 0.0385, 0.0755, 0.1004, -0.1183, -0.0660, -0.0296, -0.0135, -0.3393, -0.4640, -0.2311, -0.0445, -0.3862, -0.3265, -0.0723, -0.0786, -0.0869, -0.0812, 0.0222, 0.1255, -0.0517, -0.0829, 0.0354, -0.0078, -0.0299, 0.0393, 0.2294, 0.3138, 0.0255, 0.0508, 0.2044, 0.0263, 0.0931, 0.3356, 0.4444, 0.1022, 0.1381, 0.3155, -0.2511, -0.3291, -0.0833, -0.0242, -0.4311, -0.2519, 0.1210, 0.1685, -0.1070, -0.0091, 0.1313, 0.0389, -0.1363, -0.0312, -0.0235, -0.0087, 0.0436, -0.1155, -0.2141, -0.1155, 0.1330]
            ,[0.0597, 0.0592, 0.1732, 0.2464, -0.1685, 0.0211, 0.1856, 0.2954, -0.3482, -0.0659, 0.1850, 0.3362, -0.4334, -0.0982, -0.0953, -0.2347, -0.3696, -0.0582, -0.0391, -0.0280, 0.0198, 0.0316, -0.3851, -0.1028, 0.1100, 0.2533, -0.5882, -0.2346, 0.0717, 0.2549, -0.6017, -0.1794, -0.0862, -0.1870, -0.3976, -0.0888, -0.0011, -0.0244, -0.0639, -0.4762, -0.0915, 0.1682, 0.3427, -0.5907, -0.2490, 0.0912, 0.2848, -0.4270, -0.0716, 0.0978, 0.0253, -0.1760, 0.0619, 0.1823, -0.0517, -0.3706, -0.1937, 0.0980, 0.3040, -0.1868, -0.2315, 0.0789, 0.2590, 0.0217, 0.1580, 0.4090, 0.3120, 0.2044, 0.3575, 0.4536, -0.1617, -0.3718, -0.0546, 0.1812, 0.1609, -0.1938, 0.0519, 0.2085, 0.4415, 0.4543, 0.5960, 0.3825, 0.5265, 0.6641, 0.5857, 0.2947, 0.4674, 0.5503, -0.0171, 0.2265, 0.5127, 0.6262, 0.0838, 0.4359, 0.4795, 0.2077, 0.2001, 0.4779, 0.4764, 0.1287, 0.1639, 0.2435, -0.2358, 0.0115, 0.1146, 0.1862, 0.1157, 0.1723, 0.0664, 0.1883, 0.1867, 0.2144, 0.0292, 0.3930, -0.2254, -0.4120, -0.2702, 0.2903, -0.0026, 0.0542, 0.0308, 0.2413, 0.0891, 0.1075, 0.4594, -0.1548, -0.5858, -0.6252, 0.3366, -0.0747, -0.0482, -0.0453, 0.2619, 0.0164, 0.0145, 0.3270, 0.5627, 0.6515, 0.0789, 0.5175, 0.4908, 0.0575, 0.1644, 0.5201, 0.4279, 0.1016, 0.1364, 0.2972, 0.2492, 0.1616, 0.0078, 0.2670, 0.2493, 0.2227, 0.0317, 0.4750, 0.1727, 0.2908, 0.3077, 0.3879, 0.2015, 0.2502, 0.5537, 0.1374, 0.3143, 0.4345, 0.4474, 0.1667, 0.2456, 0.3746, 0.2494, -0.2273, 0.0758, 0.5078, 0.2902, -0.2435, -0.3262, 0.1781, 0.0085, -0.0192, -0.0744, -0.0343, -0.2798, -0.0712, -0.3974, -0.4959, -0.1966, 0.2526, 0.0279, -0.2331]
            ,[0.0252, 0.0521, 0.0734, 0.0770, -0.0010, -0.0395, 0.0089, 0.0882, -0.0132, -0.1200, -0.5998, -0.9562, -0.0034, -0.1165, -0.3785, -0.5064, 0.0208, -0.0409, -0.2316, 0.0245, 0.0310, 0.0167, 0.0454, 0.0103, 0.0654, 0.1522, 0.0610, -0.0640, -0.5508, -0.8987, 0.0830, -0.0587, -0.3140, -0.4073, 0.1119, 0.0223, -0.1569, -0.0103, -0.0392, 0.1357, 0.0818, 0.1333, 0.2202, 0.1703, 0.0249, -0.4144, -0.6932, 0.1795, 0.0349, -0.1619, -0.1986, 0.2003, 0.1074, -0.0348, -0.0372, 0.2044, 0.1597, 0.2013, 0.2858, 0.2628, 0.1417, -0.2116, -0.4044, 0.2736, 0.1593, 0.0348, 0.0223, 0.2876, 0.2138, 0.0990, 0.1813, 0.2379, 0.2733, 0.3523, 0.3086, 0.2740, 0.0128, -0.1172, 0.3598, 0.3036, 0.1995, 0.1695, 0.3666, 0.3282, 0.2030, -0.0613, -0.0468, -0.0149, 0.0173, -0.1271, -0.5469, -0.7100, 0.0260, -0.1244, -0.2543, -0.2387, 0.0488, -0.0609, -0.1541, -0.0184, -0.0232, -0.0479, -0.0345, -0.0651, -0.0622, -0.0338, -0.0772, -0.0283, 0.0164, -0.0119, -0.0866, -0.0839, -0.0094, -0.0527, -0.0457, 0.1794, 0.2458, -0.0451, -0.0899, 0.0034, 0.0569, -0.0208, -0.1104, -0.0869, -0.0350, -0.0580, 0.2804, 0.4382, -0.0314, -0.0949, 0.0124, 0.0648, -0.0045, -0.1145, -0.0833, -0.1143, -0.5252, -0.6938, 0.0047, -0.1389, -0.2925, -0.2550, 0.0207, -0.0823, -0.1853, 0.0064, -0.0142, -0.0882, -0.0672, 0.0214, 0.0386, -0.0651, -0.1114, -0.0775, -0.0186, -0.4511, -0.1932, -0.3006, -0.2559, -0.4038, -0.3220, -0.3983, -0.6606, -0.2538, -0.5027, -0.5208, -0.6381, -0.4561, -0.6061, -0.1095, -0.2658, -0.2667, 0.0132, -0.0745, -0.2351, 0.0712, 0.0386, -0.0612, -0.0588, 0.0000, -0.0288, -0.1900, -0.0175, -0.1049, -0.2462, -0.0437, -0.1194, -0.0407, -0.1730, 0.0357]])
        self.model.intercept_ = [-2.78227721, 3.52517849, -6.49817673, 5.75527545]
        self.model.classes_ = np.array(['a', 'b', 'c', 'd'])

    def predict(self, hand):
        move = self.pr.handFeatureExtracting(hand = hand, tag = None) #Will be predicted move
        tag = self.model.predict(move.handDistance)[0] #Prediction
        move.tag = tag
        return move

def main():
    random_state_pos = 42
    data = pd.read_csv("data.csv", index_col=0) #Our data which we created at handDetectorModule

    X = data.iloc[:, 0:data.iloc[0].__len__()-1] #Our data X
    y = data.loc[:,"tag"] #Our data Y

    train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=random_state_pos)
    model = LogisticRegression(random_state=random_state_pos)
    model.fit(train_X, train_y)
    predictions = model.predict(test_X)
    #To observe y_test and prediction
    test_y = pd.Series(test_y, name = "tagTest_y")
    predictions = pd.Series(predictions, index=test_y.index, name="tagPredictions")

    test_y = test_y.sort_index(axis = 0)
    predictions = predictions.sort_index(axis = 0)

    #If we want to calculate mae we should encode our target data
    encoder = LabelEncoder()
    encoder.fit(["a", "b", "c", "d"]) #target data
    encodeTest_Y = encoder.transform(test_y)
    encodePredictions = encoder.transform(predictions)

    maeFirst = mean_absolute_error(encodeTest_Y, encodePredictions)
    print(maeFirst) #You should see 0!
    table = pd.DataFrame([test_y, predictions])
    logic = [test_y == predictions]
    print(table)

if __name__ == "__main__":
    main()